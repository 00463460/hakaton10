---
title: "Chapter 4: The Digital Twin (Gazebo & Unity)"
sidebar_position: 4
sidebar_label: "Ch4: Digital Twin"
description: "Learn about URDF/SDF models, physics simulation in Gazebo, and sensor simulation for LiDAR and depth cameras in robotics digital twins."
---

# Chapter 4: The Digital Twin (Gazebo & Unity)

## Learning Objectives

By the end of this chapter, you will be able to:

- Understand the concept of digital twins in robotics and their role in simulation-based development
- Create and modify URDF (Unified Robot Description Format) files to describe robot models
- Work with SDF (Simulation Description Format) for complex multi-robot scenarios
- Configure physics engines in Gazebo for realistic robot behavior simulation
- Implement LiDAR and depth camera sensors in simulation environments
- Integrate simulated sensor data with ROS 2 for perception pipeline development

---

## 4.1 Introduction to Digital Twins

A **digital twin** is a virtual representation of a physical robot system that accurately mirrors its structure, dynamics, and sensor characteristics. In robotics development, digital twins enable engineers to test algorithms, validate designs, and train AI models in a safe, reproducible simulation environment before deploying to real hardware.

### Why Digital Twins Matter for Physical AI

The development of humanoid robots and autonomous systems requires extensive testing of navigation, manipulation, and perception algorithms. Testing these capabilities on physical robots is:

- **Expensive**: Hardware damage from failed experiments can cost thousands of dollars
- **Slow**: Physical setup and reset between experiments limits iteration speed
- **Dangerous**: Untested autonomous behavior poses safety risks to humans and equipment
- **Limited**: Real-world testing environments may not cover all edge cases

Digital twins solve these challenges by providing:

1. **Rapid Iteration**: Test thousands of scenarios in parallel at faster-than-real-time speeds
2. **Safety**: Validate dangerous maneuvers (high-speed navigation, aggressive manipulation) without risk
3. **Reproducibility**: Exact scenario replay with controlled randomization for robust algorithm development
4. **Sim-to-Real Transfer**: Pre-train AI models in simulation before fine-tuning on hardware

### The Gazebo Ecosystem

**Gazebo** (now rebranded as Gazebo Sim under Gazebo Fortress/Garden releases) is the industry-standard robotics simulator, tightly integrated with ROS 2. It provides:

- **Physics engines**: ODE, Bullet, DART, Simbody for realistic dynamics simulation
- **Sensor simulation**: LiDAR, cameras, IMU, GPS, force-torque sensors with configurable noise models
- **Rendering**: OGRE2 graphics engine for photorealistic visuals and camera sensors
- **Plugin architecture**: Extend functionality with C++ or Python plugins for custom behaviors

Gazebo is used by leading robotics teams worldwide, including Boston Dynamics (for early Spot development), NASA (Mars rover simulation), and autonomous vehicle companies (Waymo, Cruise) for sensor validation.

:::tip Real-World Application
When Boston Dynamics developed their Atlas humanoid robot, they used Gazebo extensively to test balance controllers and locomotion gaits before deploying to the $2M+ hardware. Simulation enabled testing 10,000+ fall scenarios to make the controller robust without damaging the physical robot.
:::

### Unity for Robotics Simulation

While Gazebo excels at physics and sensor simulation, **Unity** (with the Unity Robotics Hub) offers complementary strengths:

- **Photorealistic rendering**: Ideal for training computer vision models with domain randomization
- **Large-scale environments**: Efficiently simulate warehouse-scale robot fleets (100+ agents)
- **VR/AR integration**: Develop human-robot interaction interfaces with immersive visualization

This chapter focuses primarily on Gazebo due to its mature ROS 2 integration, but concepts (URDF models, sensor configuration) apply to both platforms.

[Image of Digital Twin Concept: Split-screen showing physical humanoid robot on left, Gazebo simulation of same robot on right, with sensor data overlays demonstrating parity between real and simulated environments]

---

## 4.2 URDF File Structure

The **Unified Robot Description Format (URDF)** is an XML-based language for describing robot kinematics, dynamics, and visual/collision geometry. Every ROS 2 robot requires a URDF file to enable tools like RViz (visualization), MoveIt (motion planning), and Gazebo (simulation) to understand the robot's structure.

### Anatomy of a URDF File

A URDF file is composed of three primary elements:

1. **Links**: Rigid bodies representing robot parts (chassis, arms, wheels, sensors)
2. **Joints**: Connections between links defining motion constraints (revolute, prismatic, fixed)
3. **Transmissions**: (Optional) Map joint states to actuators for control

Each link contains:
- **Visual**: Mesh or primitive geometry for rendering in RViz/Gazebo
- **Collision**: Simplified geometry for physics collision detection (typically simpler than visual for performance)
- **Inertial**: Mass and inertia tensor for physics simulation

### Coordinate Transforms in URDF

URDF links are connected via joints that define relative transformations. The transformation from parent link frame to child link frame is computed as:

$$
T_{child}^{world} = T_{parent}^{world} \cdot T_{origin} \cdot R_{axis}(\theta)
$$

Where:
- $T_{parent}^{world}$: Parent link pose in world frame
- $T_{origin}$: Fixed translation/rotation offset defined in joint's `<origin>` tag
- $R_{axis}(\theta)$: Rotation about joint axis by angle $\theta$ (for revolute joints)

### Example: Simple Mobile Robot URDF

```xml
<?xml version="1.0"?>
<robot name="simple_mobile_robot">

  <!-- Base link (chassis) -->
  <link name="base_link">
    <visual>
      <geometry>
        <box size="0.6 0.4 0.2"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 0.8 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.6 0.4 0.2"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="10.0"/>
      <inertia ixx="0.4" ixy="0.0" ixz="0.0"
               iyy="0.6" iyz="0.0" izz="0.8"/>
    </inertial>
  </link>

  <!-- Left wheel -->
  <link name="left_wheel">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.0"/>
      <inertia ixx="0.01" ixy="0.0" ixz="0.0"
               iyy="0.01" iyz="0.0" izz="0.01"/>
    </inertial>
  </link>

  <!-- Joint connecting base to left wheel -->
  <joint name="left_wheel_joint" type="continuous">
    <parent link="base_link"/>
    <child link="left_wheel"/>
    <origin xyz="0 0.225 0" rpy="-1.5708 0 0"/>
    <axis xyz="0 0 1"/>
  </joint>

  <!-- Right wheel (similar to left, mirrored) -->
  <link name="right_wheel">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.0"/>
      <inertia ixx="0.01" ixy="0.0" ixz="0.0"
               iyy="0.01" iyz="0.0" izz="0.01"/>
    </inertial>
  </link>

  <joint name="right_wheel_joint" type="continuous">
    <parent link="base_link"/>
    <child link="right_wheel"/>
    <origin xyz="0 -0.225 0" rpy="-1.5708 0 0"/>
    <axis xyz="0 0 1"/>
  </joint>

</robot>
```

**Key Observations**:
- The `<inertia>` tensor must be positive-definite; Gazebo will issue warnings if violated
- `type="continuous"` joints rotate indefinitely (wheels), vs. `revolute` (limited angle) or `prismatic` (linear)
- `rpy="roll pitch yaw"` uses radians; `-1.5708` rad = -90° to orient wheel cylinder correctly

### Loading URDF in ROS 2

```python
import rclpy
from rclpy.node import Node
from ament_index_python.packages import get_package_share_directory
import os

class RobotDescriptionPublisher(Node):
    def __init__(self):
        super().__init__('robot_description_publisher')

        # Load URDF file from package share directory
        urdf_file = os.path.join(
            get_package_share_directory('my_robot_description'),
            'urdf',
            'simple_mobile_robot.urdf'
        )

        with open(urdf_file, 'r') as file:
            robot_description = file.read()

        # Publish to /robot_description topic for other nodes
        self.declare_parameter('robot_description', robot_description)
        self.get_logger().info('URDF loaded and published to /robot_description')

def main(args=None):
    rclpy.init(args=args)
    node = RobotDescriptionPublisher()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

This pattern is standard in ROS 2: a node publishes the URDF string to the `/robot_description` parameter, which RViz, MoveIt, and Gazebo read to visualize/simulate the robot.

[Image of URDF Link-Joint Tree: Hierarchical diagram showing base_link as root, with left_wheel and right_wheel as children connected via revolute joints, including coordinate frame axes (X-red, Y-green, Z-blue) at each link origin]

:::warning Common Pitfall
Forgetting to define inertial properties causes Gazebo to treat links as massless, leading to unstable physics. Always specify realistic mass and inertia values, even for simple prototypes.
:::

---

## 4.3 SDF Models and World Files

While URDF is sufficient for single-robot descriptions, **SDF (Simulation Description Format)** extends capabilities for complex multi-robot scenarios and world environments. SDF is Gazebo's native format and supports features unavailable in URDF:

- **Multiple models per file**: Define entire warehouse with robots, obstacles, and fixtures in one file
- **Nested models**: Compose robots from reusable sub-assemblies (e.g., gripper as nested model)
- **Plugin configuration**: Attach Gazebo plugins directly in SDF without external launch files
- **Advanced physics**: Configure friction models, contact properties, and solver parameters per-model

### SDF World File Structure

An SDF world file (`.world` or `.sdf`) defines:
1. **Physics engine settings**: Gravity, timestep, solver iterations
2. **Scene lighting**: Sun position, shadows, ambient occlusion
3. **Models**: Robots, static obstacles, spawnable objects
4. **Plugins**: Custom behaviors (e.g., spawn robots programmatically, simulate conveyor belts)

### Example: Warehouse Environment SDF

```xml
<?xml version="1.0"?>
<sdf version="1.9">
  <world name="warehouse_world">

    <!-- Physics engine configuration -->
    <physics name="ode_physics" type="ode">
      <max_step_size>0.001</max_step_size>
      <real_time_factor>1.0</real_time_factor>
      <real_time_update_rate>1000</real_time_update_rate>
      <gravity>0 0 -9.81</gravity>
    </physics>

    <!-- Lighting -->
    <light name="sun" type="directional">
      <cast_shadows>true</cast_shadows>
      <pose>0 0 10 0 0 0</pose>
      <diffuse>1.0 1.0 1.0 1</diffuse>
      <specular>0.2 0.2 0.2 1</specular>
      <direction>-0.5 0.1 -0.9</direction>
    </light>

    <!-- Ground plane -->
    <model name="ground_plane">
      <static>true</static>
      <link name="link">
        <collision name="collision">
          <geometry>
            <plane>
              <normal>0 0 1</normal>
              <size>100 100</size>
            </plane>
          </geometry>
          <surface>
            <friction>
              <ode>
                <mu>100</mu>  <!-- High friction for stable wheel contact -->
                <mu2>50</mu2>
              </ode>
            </friction>
          </surface>
        </collision>
        <visual name="visual">
          <geometry>
            <plane>
              <normal>0 0 1</normal>
              <size>100 100</size>
            </plane>
          </geometry>
          <material>
            <ambient>0.8 0.8 0.8 1</ambient>
            <diffuse>0.8 0.8 0.8 1</diffuse>
          </material>
        </visual>
      </link>
    </model>

    <!-- Warehouse wall (example of static obstacle) -->
    <model name="warehouse_wall_north">
      <static>true</static>
      <pose>10 0 1.5 0 0 0</pose>
      <link name="link">
        <collision name="collision">
          <geometry>
            <box>
              <size>0.2 20 3</size>
            </box>
          </geometry>
        </collision>
        <visual name="visual">
          <geometry>
            <box>
              <size>0.2 20 3</size>
            </box>
          </geometry>
          <material>
            <ambient>0.5 0.5 0.5 1</ambient>
          </material>
        </visual>
      </link>
    </model>

    <!-- Include mobile robot model (references external URDF converted to SDF) -->
    <include>
      <uri>model://simple_mobile_robot</uri>
      <name>robot1</name>
      <pose>0 0 0.1 0 0 0</pose>
    </include>

  </world>
</sdf>
```

**Key SDF Features**:
- `<physics>` tag centralizes solver settings; `max_step_size=0.001` gives 1ms timestep (1kHz simulation rate)
- `<surface><friction><ode><mu>` defines friction coefficient; 100 is high (rubber on concrete), 0.1 is low (ice)
- `<include>` references pre-built models from Gazebo's model database or local paths

### Converting URDF to SDF

Gazebo automatically converts URDF to SDF at runtime, but explicit conversion gives more control:

```bash
# Using gz command-line tool (Gazebo Garden/Harmonic)
gz sdf -p simple_mobile_robot.urdf > simple_mobile_robot.sdf

# Manually add Gazebo plugins to SDF after conversion
```

[Image of SDF World Hierarchy: Tree diagram showing world file containing physics config, lighting, ground plane, multiple wall models, and two robot instances (robot1, robot2) with different spawn poses]

---

## 4.4 Physics Simulation in Gazebo

Accurate physics simulation is critical for developing robust control algorithms. Gazebo supports multiple physics engines, each with trade-offs between speed and fidelity.

### Physics Engine Comparison

| Engine | Strengths | Weaknesses | Use Cases |
|--------|-----------|------------|-----------|
| **ODE** (default) | Fast, stable for wheeled robots | Less accurate for complex contacts | Mobile robots, simple manipulators |
| **Bullet** | Good multi-contact handling | Can be slow with many objects | Humanoid walking, grasping |
| **DART** | High-fidelity dynamics, inverse dynamics | Slower than ODE | Research, biomechanics |
| **Simbody** | Accurate constraint satisfaction | Limited adoption, compatibility | Medical robotics |

For most humanoid robotics applications, **Bullet** or **DART** are recommended due to superior multi-contact support (foot-ground interactions during walking).

### Configuring Physics Parameters

The physics simulation quality depends on:

1. **Timestep (`max_step_size`)**: Smaller = more accurate but slower. Typical range: 0.001s (1kHz) to 0.01s (100Hz)
2. **Solver iterations (`iters`)**: More iterations = better constraint satisfaction. Default 50, increase to 100+ for humanoids
3. **Contact parameters**: Max contacts, friction, bounce (restitution)

$$
F_{friction} \leq \mu \cdot F_{normal}
$$

Where $\mu$ is the friction coefficient (SDF's `<mu>` tag). For a robot foot on concrete, $\mu \approx 0.8$.

### Physics-Realistic Robot Spawner

```python
import rclpy
from rclpy.node import Node
from gazebo_msgs.srv import SpawnEntity
import os

class GazeboRobotSpawner(Node):
    def __init__(self):
        super().__init__('gazebo_robot_spawner')

        # Create client for Gazebo spawn service
        self.spawn_client = self.create_client(SpawnEntity, '/spawn_entity')

        while not self.spawn_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Waiting for /spawn_entity service...')

        # Load SDF model
        sdf_file_path = os.path.join(
            os.path.dirname(__file__),
            '..',
            'models',
            'humanoid_robot.sdf'
        )

        with open(sdf_file_path, 'r') as sdf_file:
            robot_description_sdf = sdf_file.read()

        # Spawn robot at origin with slight elevation to avoid ground collision
        self.spawn_robot(
            name='humanoid_1',
            xml=robot_description_sdf,
            initial_pose={'x': 0.0, 'y': 0.0, 'z': 0.85}  # 85cm elevation
        )

    def spawn_robot(self, name, xml, initial_pose):
        request = SpawnEntity.Request()
        request.name = name
        request.xml = xml
        request.robot_namespace = name
        request.initial_pose.position.x = initial_pose['x']
        request.initial_pose.position.y = initial_pose['y']
        request.initial_pose.position.z = initial_pose['z']

        future = self.spawn_client.call_async(request)
        rclpy.spin_until_future_complete(self, future)

        if future.result() is not None:
            self.get_logger().info(f'Successfully spawned {name}')
        else:
            self.get_logger().error(f'Failed to spawn {name}')

def main(args=None):
    rclpy.init(args=args)
    spawner = GazeboRobotSpawner()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Why spawn at `z=0.85` instead of `z=0`?** Spawning at ground level (`z=0`) can cause initial penetration between robot feet and ground plane, triggering explosive contact forces. Spawning slightly elevated allows gravity to settle the robot naturally onto the ground.

:::tip Performance Optimization
If simulation runs slower than real-time (`real_time_factor < 1.0`), reduce physics complexity:
1. Simplify collision geometries (use boxes/cylinders instead of meshes)
2. Increase timestep from 0.001s to 0.005s (acceptable for mobile robots)
3. Reduce `max_contacts` from default 20 to 10 per collision pair
4. Disable shadows in lighting configuration
:::

---

## 4.5 Sensor Simulation: LiDAR and Depth Cameras

Simulated sensors generate synthetic data that feeds perception algorithms, enabling pure-sim development before hardware availability.

### LiDAR (Light Detection and Ranging)

LiDAR sensors emit laser pulses and measure time-of-flight to compute range (distance) to obstacles. They produce **point clouds**: 3D coordinates $(x, y, z)$ of reflected surfaces.

Gazebo simulates LiDAR via ray-casting: shoot rays in a configured pattern (360° horizontal sweep, vertical layers) and record intersection distances. A realistic LiDAR noise model adds Gaussian error:

$$
d_{measured} = d_{true} + \mathcal{N}(0, \sigma^2)
$$

Where $\sigma$ depends on range (typical: $\sigma = 0.01 \cdot d_{true}$ for 1% error).

### Adding LiDAR to Robot SDF

```xml
<!-- Add this inside a <link> tag in your robot's SDF -->
<sensor name="lidar_sensor" type="gpu_lidar">
  <pose>0 0 0.3 0 0 0</pose>  <!-- 30cm above link origin -->
  <update_rate>10</update_rate>  <!-- 10 Hz -->
  <lidar>
    <scan>
      <horizontal>
        <samples>360</samples>  <!-- 360 rays = 1° resolution -->
        <resolution>1</resolution>
        <min_angle>-3.14159</min_angle>  <!-- -180° -->
        <max_angle>3.14159</max_angle>   <!-- +180° -->
      </horizontal>
      <vertical>
        <samples>16</samples>  <!-- 16-layer LiDAR (e.g., Velodyne VLP-16) -->
        <resolution>1</resolution>
        <min_angle>-0.2618</min_angle>  <!-- -15° -->
        <max_angle>0.2618</max_angle>   <!-- +15° -->
      </vertical>
    </scan>
    <range>
      <min>0.1</min>  <!-- 10cm minimum range -->
      <max>100.0</max>  <!-- 100m maximum range -->
      <resolution>0.01</resolution>  <!-- 1cm range resolution -->
    </range>
    <noise>
      <type>gaussian</type>
      <mean>0.0</mean>
      <stddev>0.01</stddev>  <!-- 1cm standard deviation -->
    </noise>
  </lidar>

  <!-- Plugin to publish to ROS 2 -->
  <plugin name="lidar_plugin" filename="libgazebo_ros_ray_sensor.so">
    <ros>
      <namespace>/humanoid_1</namespace>
      <remapping>~/out:=scan</remapping>
    </ros>
    <output_type>sensor_msgs/PointCloud2</output_type>
    <frame_name>lidar_link</frame_name>
  </plugin>
</sensor>
```

This publishes a `sensor_msgs/PointCloud2` message to `/humanoid_1/scan` at 10 Hz.

### Depth Camera Simulation

Depth cameras (e.g., Intel RealSense, Kinect) provide RGB images + per-pixel depth. Gazebo renders a depth buffer from the camera viewpoint and converts to ROS messages.

### Processing LiDAR Data in ROS 2

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2
import sensor_msgs_py.point_cloud2 as pc2
import numpy as np

class LidarProcessor(Node):
    def __init__(self):
        super().__init__('lidar_processor')

        self.subscription = self.create_subscription(
            PointCloud2,
            '/humanoid_1/scan',
            self.lidar_callback,
            10
        )

        self.get_logger().info('LiDAR processor initialized')

    def lidar_callback(self, msg):
        # Convert PointCloud2 to numpy array
        points = pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True)
        points_array = np.array(list(points))

        # Simple obstacle detection: find points within 2m
        distances = np.linalg.norm(points_array, axis=1)
        close_points = points_array[distances < 2.0]

        if len(close_points) > 10:  # Threshold: 10+ points means obstacle
            self.get_logger().warn(f'Obstacle detected! {len(close_points)} points within 2m')

            # Compute centroid of close points (obstacle center)
            centroid = np.mean(close_points, axis=0)
            self.get_logger().info(f'Obstacle centroid: x={centroid[0]:.2f}, y={centroid[1]:.2f}, z={centroid[2]:.2f}')

def main(args=None):
    rclpy.init(args=args)
    processor = LidarProcessor()
    rclpy.spin(processor)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

This node subscribes to the LiDAR point cloud, filters points within 2 meters, and computes the obstacle centroid—a basic building block for navigation algorithms.

[Image of LiDAR Point Cloud Visualization: 3D scatter plot showing point cloud data from a simulated LiDAR sensor, with color gradient from blue (close) to red (far) based on distance, overlaid on a wireframe warehouse environment]

[Image of Depth Camera Output: Side-by-side comparison of RGB image (left) and corresponding depth map (right) from a simulated RealSense camera, with depth encoded as grayscale where darker = closer]

[Image of Sensor Noise Visualization: Graph plotting true distance (x-axis) vs. measured distance with Gaussian noise (y-axis, scatter points around y=x diagonal line), demonstrating $\sigma = 0.01 \cdot d_{true}$ noise model with increasing spread at longer ranges]

:::info Domain Randomization for Sim-to-Real Transfer
To train perception models that generalize to real hardware, apply **domain randomization**:
- Vary lighting conditions in SDF world files (sun intensity, shadows on/off)
- Randomize surface textures and colors using Gazebo material scripts
- Add temporal noise to sensor data (simulate electrical interference)
- Randomize sensor mount positions within manufacturing tolerances (±2mm translation, ±0.5° rotation)

This prevents overfitting to the "perfect" simulation and improves real-world performance by 30-50% (empirical results from sim-to-real literature).
:::

---

## Summary

In this chapter, we explored the foundations of digital twin technology for robotics:

- **Digital twins** enable safe, rapid iteration for algorithm development before deploying to expensive hardware
- **URDF** describes robot kinematics and geometry for ROS 2 visualization and control
- **SDF** extends URDF for multi-robot worlds and advanced physics configuration
- **Gazebo physics engines** (ODE, Bullet, DART) provide realistic dynamics with tunable accuracy/speed trade-offs
- **Sensor simulation** (LiDAR, depth cameras) generates synthetic data for perception pipeline development

### Key Takeaways

1. Always define realistic inertial properties in URDF/SDF to avoid unstable physics
2. Choose physics engine based on contact complexity: ODE for wheeled robots, Bullet/DART for humanoids
3. Spawn robots with slight elevation (`z > 0`) to prevent ground penetration explosions
4. Add Gaussian noise to simulated sensors ($\sigma \approx 1\%$ of measurement) for realistic perception testing
5. Use domain randomization (lighting, textures, sensor noise) to improve sim-to-real transfer

### Next Steps

- **Chapter 5**: Advanced Gazebo plugins for custom robot behaviors (differential drive controllers, joint trajectory execution)
- **Chapter 6**: Vision-Language-Action (VLA) models for humanoid task learning using simulated sensor data
- **Chapter 7**: Deploying sim-trained policies to real robots with calibration and fine-tuning strategies

---

## Practice Exercises

1. **URDF Extension**: Modify the `simple_mobile_robot.urdf` to add a LiDAR sensor link mounted at height 0.3m above the base_link. Define the sensor's visual geometry as a small cylinder (radius 0.05m, length 0.1m).

2. **SDF World Building**: Create a warehouse world SDF with at least 5 static obstacles (walls, boxes) and spawn two mobile robots at different starting positions. Configure lighting to cast shadows.

3. **Physics Comparison**: Spawn the same robot in three separate Gazebo instances using ODE, Bullet, and DART physics engines. Apply identical forces to a wheel joint and measure settling time. Which engine stabilizes fastest?

4. **LiDAR Processing**: Extend the `LidarProcessor` node to implement a simple wall-following behavior: if obstacle detected within 1.5m on the left, turn right; if on right, turn left. Publish velocity commands to `/cmd_vel`.

5. **Sensor Noise Analysis**: Record 1000 LiDAR scans of a static wall at known distance (e.g., 5m). Plot histogram of measured distances and compute mean and standard deviation. Does it match the configured $\sigma = 0.01 \cdot d$ model?

---

## Further Reading

- [Gazebo Official Documentation](https://gazebosim.org/docs) - Comprehensive guides for Gazebo Garden/Harmonic
- [ROS 2 URDF Tutorials](https://docs.ros.org/en/humble/Tutorials/Intermediate/URDF/URDF-Main.html) - Building and visualizing robot models
- [SDF Specification](http://sdformat.org/spec) - Full SDF format reference with all available tags
- Koenig, N., & Howard, A. (2004). "Design and use paradigms for Gazebo, an open-source multi-robot simulator." *IEEE/RSJ IROS* - Original Gazebo research paper
- Peng, X. B., et al. (2018). "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization." *ICRA* - Foundational work on domain randomization for sim-to-real transfer
